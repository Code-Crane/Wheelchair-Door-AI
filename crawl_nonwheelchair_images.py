import os
import ssl
import requests
from PIL import Image
from io import BytesIO
from icrawler import ImageDownloader
from icrawler.builtin import GoogleImageCrawler, BingImageCrawler

ssl._create_default_https_context = ssl._create_unverified_context

save_dir = './dataset/nonwheelchair_raw'
os.makedirs(save_dir, exist_ok=True)

# ê³ ì • ì¸ë±ì‹± ì‹œì‘
current_index = len(os.listdir(save_dir))
target_per_keyword = 120  # 60 Google + 60 Bing

# 24ê°œ í‚¤ì›Œë“œ
keywords = [
    # ì„¸ë°œìì „ê±°
    "ì„¸ë°œìì „ê±°", "tricycle", "ä¸‰è¼ªè»Š", "ä¸‰è½®è½¦",
    # í‚¥ë³´ë“œ
    "í‚¥ë³´ë“œ", "kickboard", "ã‚­ãƒƒã‚¯ãƒœãƒ¼ãƒ‰", "æ»‘æ¿è½¦",
    # ì¸ë ¥ê±°
    "ì¸ë ¥ê±°", "rickshaw", "äººåŠ›è»Š", "é»„åŒ…è½¦",
    # ì§€ê²Œì°¨
    "ì§€ê²Œì°¨", "forklift", "ãƒ•ã‚©ãƒ¼ã‚¯ãƒªãƒ•ãƒˆ", "å‰è½¦",
    # ìì „ê±°
    "ìì „ê±°", "bicycle", "è‡ªè»¢è»Š", "è‡ªè¡Œè½¦",
    # ì¶”ê°€ í‚¤ì›Œë“œ
    "ì•„ê¸°ìë™ì°¨", "ë¶•ë¶•ì¹´", "ìœ ì•„ìë™ì°¨", "ë¯¸ë‹ˆì¹´"
]

# ë‹¤ìš´ë¡œë” ì •ì˜
class IndexedDownloader(ImageDownloader):
    def download(self, task, default_ext, timeout=5, **kwargs):
        global current_index
        try:
            resp = requests.get(task['file_url'], stream=True, timeout=timeout, headers={
                "User-Agent": "Mozilla/5.0"
            })
            img = Image.open(BytesIO(resp.content))
            if img.width < 128 or img.height < 128:
                return
            ext = '.' + img.format.lower()
            filename = f"nonwheel_{str(current_index + 1).zfill(5)}{ext}"
            path = os.path.join(save_dir, filename)
            img.convert('RGB').save(path)
            current_index += 1
            print(f"[{current_index}] ì €ì¥ë¨ â†’ {filename}")
        except Exception:
            return

# í¬ë¡¤ëŸ¬ ì‹¤í–‰ í•¨ìˆ˜
def run_crawler(CrawlerClass, engine_name, num_per_engine):
    for keyword in keywords:
        print(f"\nğŸ” [{engine_name}] '{keyword}' ìˆ˜ì§‘ ì¤‘...")
        crawler = CrawlerClass(
            downloader_cls=IndexedDownloader,
            storage={'root_dir': save_dir}
        )
        try:
            crawler.crawl(
                keyword=keyword,
                max_num=num_per_engine,
                overwrite=False,
                filters=None
            )
        except Exception as e:
            print(f"âŒ {keyword} ì‹¤íŒ¨: {e}")
        print(f"ğŸ”¸ ëˆ„ì : {current_index}ì¥\n")

# ì‹¤í–‰
run_crawler(GoogleImageCrawler, "Google", 60)
run_crawler(BingImageCrawler, "Bing", 60)

print(f"\nâœ… ë…¼íœ ì²´ì–´ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ! ì´ ìˆ˜ì§‘: {current_index}ì¥")
