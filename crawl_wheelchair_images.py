import os
import ssl
import requests
from PIL import Image
from io import BytesIO
from icrawler import ImageDownloader
from icrawler.builtin import GoogleImageCrawler, BingImageCrawler

ssl._create_default_https_context = ssl._create_unverified_context

save_dir = './dataset/wheelchair_raw'
os.makedirs(save_dir, exist_ok=True)

current_index = len(os.listdir(save_dir))
target_per_keyword = 120

keywords = [
    # í•œêµ­ì–´
    "íœ ì²´ì–´", "ì „ë™ íœ ì²´ì–´", "ìˆ˜ë™ íœ ì²´ì–´", "ë³‘ì› íœ ì²´ì–´", "ì¥ì• ì¸ íœ ì²´ì–´", "íœ ì²´ì–´ ì‚¬ìš©ì", "ë…¸ì•½ì íœ ì²´ì–´",
    # ì˜ì–´
    "wheelchair", "electric wheelchair", "manual wheelchair", "hospital wheelchair",
    "disabled person wheelchair", "wheelchair user", "elderly wheelchair",
    # ì¼ë³¸ì–´
    "è»Šæ¤…å­", "é›»å‹•è»Šæ¤…å­", "æ‰‹å‹•è»Šæ¤…å­", "ç—…é™¢ç”¨è»Šæ¤…å­", "éšœãŒã„è€… è»Šæ¤…å­", "è»Šæ¤…å­ ãƒ¦ãƒ¼ã‚¶ãƒ¼", "é«˜é½¢è€… è»Šæ¤…å­",
    # ì¤‘êµ­ì–´
    "è½®æ¤…", "ç”µåŠ¨è½®æ¤…", "æ‰‹åŠ¨è½®æ¤…", "åŒ»é™¢è½®æ¤…", "æ®‹ç–¾äººè½®æ¤…", "åè½®æ¤…çš„äºº", "è€å¹´äººè½®æ¤…"
]

class IndexedDownloader(ImageDownloader):
    def download(self, task, default_ext, timeout=5, **kwargs):
        global current_index
        try:
            resp = requests.get(task['file_url'], stream=True, timeout=timeout, headers={
                "User-Agent": "Mozilla/5.0"
            })
            img = Image.open(BytesIO(resp.content))
            if img.width < 128 or img.height < 128:
                return
            ext = '.' + img.format.lower()
            filename = f"wheelchair_{str(current_index + 1).zfill(5)}{ext}"
            path = os.path.join(save_dir, filename)
            img.convert('RGB').save(path)
            current_index += 1
            print(f"[{current_index}] ì €ì¥ë¨ â†’ {filename}")
        except Exception:
            return

def run_crawler(CrawlerClass, engine_name, num_per_engine):
    for keyword in keywords:
        print(f"\nğŸ” [{engine_name}] '{keyword}' ìˆ˜ì§‘ ì¤‘...")
        crawler = CrawlerClass(
            downloader_cls=IndexedDownloader,
            storage={'root_dir': save_dir}
        )
        try:
            crawler.crawl(
                keyword=keyword,
                max_num=num_per_engine,
                overwrite=False,
                filters=None
            )
        except Exception as e:
            print(f"âŒ {keyword} ì‹¤íŒ¨: {e}")
        print(f"ğŸ”¸ ëˆ„ì : {current_index}ì¥\n")

run_crawler(GoogleImageCrawler, "Google", 60)
run_crawler(BingImageCrawler, "Bing", 60)

print(f"\nâœ… íœ ì²´ì–´ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ! ì´ ìˆ˜ì§‘: {current_index}ì¥")
